# PandaAI v2 Model Registry
# Simplified Cognitive Stack Configuration (vLLM Tested)
# Target Hardware: RTX 3090 (24GB VRAM)
#
# SIMPLIFIED DESIGN (based on vLLM testing 2026-01-06):
# - Single MIND model handles ALL text roles (REFLEX, NERVES, MIND, VOICE)
# - Different temperatures per task (0.4 classification, 0.6 reasoning, 0.7 dialogue)
# - EYES swaps in on-demand for vision (requires MIND unload, 3-5s swap)
# - Embeddings run on CPU (no VRAM impact)
#
# KEY FINDING: Qwen3-0.6B (REFLEX) is NOT used - MIND handles those tasks.

version: "2.0"
created: "2026-01-04"
updated: "2026-01-06"

# =============================================================================
# VRAM BUDGET (Simplified - vLLM Tested)
# =============================================================================
hardware:
  gpu: "RTX 3090 (24GB VRAM)"
  vram_total_gb: 24
  vram_hot_pool_gb: 18.0  # Single MIND model (Qwen3-Coder-30B-AWQ)
  vram_cold_reserve_gb: 5.0  # EYES when swapped in
  context_default: 4096  # vLLM tested max_model_len
  context_ceiling: 32768  # Can increase with 24GB VRAM

# =============================================================================
# MODEL STACK (Simplified - Single MIND for All Text)
# =============================================================================
models:
  # ---------------------------------------------------------------------------
  # EMBEDDING - Semantic search (runs on CPU, no VRAM impact)
  # ---------------------------------------------------------------------------
  embedding:
    model_id: "sentence-transformers/all-MiniLM-L6-v2"
    local_path: "models/all-MiniLM-L6-v2"
    name: "all-MiniLM-L6-v2"
    format: "f32"
    type: "embedding"
    dimensions: 384
    max_seq_length: 256
    pool: "cpu"  # Runs on CPU, not VRAM
    capabilities:
      - semantic_search
      - document_retrieval
      - similarity_matching

  # ---------------------------------------------------------------------------
  # MIND - Single model for ALL text roles (vLLM tested)
  # Handles: classification, reasoning, planning, dialogue (via temperature)
  # Quantization: AWQ 4-bit
  # ---------------------------------------------------------------------------
  mind:
    model_id: "Qwen/Qwen3-Coder-30B-AWQ"
    local_path: "models/Qwen3-Coder-30B-AWQ"
    name: "Qwen3-Coder-30B-AWQ"
    format: "awq"  # AWQ 4-bit quantization
    vram_gb: 18.0  # Fits in RTX 3090 24GB VRAM
    pool: "hot"
    served_name: "mind"  # vLLM --served-model-name
    capabilities:
      # REFLEX role (classification, fast gates)
      - classification
      - reference_resolution
      - binary_decisions
      # NERVES role (background compression)
      - context_compression
      - memory_compression
      - state_tracking
      # MIND role (reasoning, planning)
      - planning
      - reasoning
      - tool_selection
      - validation
      # VOICE role (user dialogue)
      - natural_language_generation
      - conversational_output
      - formatting
    phases:
      - phase0_query_analyzer   # REFLEX role, temp=0.4
      - phase1_reflection       # REFLEX role, temp=0.4
      - phase2_context_gatherer # MIND role, temp=0.6
      - phase3_planner          # MIND role, temp=0.6
      - phase4_executor         # MIND role, temp=0.6
      - phase5_coordinator      # MIND role, temp=0.6
      - phase6_synthesis        # VOICE role, temp=0.7
      - phase7_validation       # MIND role, temp=0.6
      - background_compression  # NERVES role, temp=0.3
    # Temperature per role (applied via recipe/prompt selection)
    role_temperatures:
      reflex: 0.4    # Classification, binary decisions
      nerves: 0.3    # Compression (low creativity)
      mind: 0.6      # Reasoning, planning
      voice: 0.7     # User dialogue (more natural)
    parameters:
      temperature: 0.6  # Default (overridden per role)
      max_tokens: 2000
      context_window: 4096  # vLLM tested max_model_len
    notes: |
      Single model serves all text roles. Role behavior controlled by:
      - Temperature (0.4 for classification, 0.6 for reasoning, 0.7 for dialogue)
      - System prompts (role-specific instructions)
      REFLEX (Qwen3-0.6B) is NOT used - MIND handles those tasks.

  # ---------------------------------------------------------------------------
  # EYES - Vision tasks (cold pool, swaps with MIND)
  # Kept as BF16 for quality - requires unloading MIND temporarily
  # ---------------------------------------------------------------------------
  eyes:
    model_id: "Qwen/Qwen3-VL-2B-Instruct"
    local_path: "models/Qwen3-VL-2B-Instruct"
    name: "Qwen3-VL-2B-Instruct"
    format: "bf16"  # Kept as BF16 for vision quality
    vram_gb: 5.0  # Estimated 4-5GB
    pool: "cold"
    capabilities:
      - screenshot_analysis
      - captcha_detection
      - ui_element_identification
      - image_understanding
    phases:
      - phase4_vision_subtask
    parameters:
      temperature: 0.3
      max_tokens: 1000
      context_window: 4096
    swap_strategy: "unload_mind"  # Must unload MIND to load EYES
    swap_time_seconds: 5  # 3-5 second cold load
    notes: |
      Vision model swaps with MIND (not REFLEX - REFLEX is not used).
      Swap process: unload MIND -> load EYES -> execute vision -> unload EYES -> reload MIND
      Total swap overhead: ~5-10 seconds per vision task.

  # ---------------------------------------------------------------------------
  # SERVER - Remote heavy coding model
  # Runs on separate server, accessed via API
  # ---------------------------------------------------------------------------
  server:
    model_id: "Qwen/Qwen3-Coder-30B-AWQ"
    name: "Qwen3-Coder-30B-AWQ"
    pool: "remote"
    type: "remote"
    endpoint: "${SERVER_ENDPOINT:-http://localhost:8001}"  # Configure in .env
    capabilities:
      - complex_code_generation
      - large_refactoring
      - multi_file_changes
      - code_review
      - algorithm_implementation
    parameters:
      temperature: 0.3
      max_tokens: 8192
      context_window: 32768
    triggers:
      - code_complexity_high
      - multi_file_edit
      - algorithm_request
    fallback: "mind"  # Use local MIND if server unavailable
    notes: "Remote server running Qwen3-Coder-30B-AWQ for heavy coding tasks"

# =============================================================================
# POOL CONFIGURATION (Simplified)
# =============================================================================
pools:
  hot:
    description: "Always loaded in VRAM"
    models:
      - mind  # Qwen3-Coder-30B-AWQ (~18GB) - handles ALL text roles
    # Note: REFLEX (Qwen3-0.6B) is NOT used - MIND handles those tasks
    total_vram_gb: 18.0

  cold:
    description: "Loaded on demand, unloaded after use"
    models:
      - eyes  # Qwen3-VL-2B: ~5GB
    swap_strategy: "unload_mind"  # Must unload MIND to load EYES
    max_vram_gb: 5.0

  cpu:
    description: "Runs on CPU, no VRAM impact"
    models:
      - embedding  # all-MiniLM-L6-v2

  remote:
    description: "Remote server models accessed via API"
    models:
      - server  # Qwen3-Coder-30B-AWQ on remote server
    connection:
      type: "http"
      timeout_seconds: 120
      retry_count: 2
    fallback_pool: "hot"  # Fall back to local MIND if remote unavailable

# =============================================================================
# PHASE-TO-MODEL MAPPING (All phases use MIND with different temperatures)
# =============================================================================
phase_assignments:
  phase0_query_analyzer:
    model: mind
    role: reflex  # Use REFLEX temperature (0.4)
    recipe: "query_analyzer.yaml"

  phase1_reflection:
    model: mind
    role: reflex  # Use REFLEX temperature (0.4)
    recipe: "reflection.yaml"

  phase2_context_gatherer:
    model: mind
    role: mind  # Use MIND temperature (0.6)
    recipes:
      - "context_gatherer_retrieval.yaml"
      - "context_gatherer_synthesis.yaml"

  phase3_planner:
    model: mind
    role: mind  # Use MIND temperature (0.6)
    recipes:
      chat: "planner_chat.yaml"
      code: "planner_code.yaml"

  phase4_executor:
    model: mind
    role: mind  # Use MIND temperature (0.6)
    recipes:
      chat: "coordinator_chat.yaml"
      code: "coordinator_code.yaml"
    vision_subtask: eyes  # Swap to EYES for vision tasks

  phase5_coordinator:
    model: mind
    role: mind  # Use MIND temperature (0.6)
    recipes:
      chat: "coordinator_chat.yaml"
      code: "coordinator_code.yaml"

  phase6_synthesis:
    model: mind
    role: voice  # Use VOICE temperature (0.7)
    recipes:
      chat: "synthesizer_chat.yaml"
      code: "synthesizer_code.yaml"

  phase7_validation:
    model: mind
    role: mind  # Use MIND temperature (0.6)
    recipe: "validator.yaml"

  phase8_save:
    model: null  # Procedural, no LLM

# =============================================================================
# BACKGROUND SERVICES (All use MIND with NERVES temperature)
# =============================================================================
background_services:
  context_compression:
    model: mind
    role: nerves  # Use NERVES temperature (0.3)
    trigger: "section_4_exceeds_budget"
    recipe: "compression.yaml"

  memory_compression:
    model: mind
    role: nerves  # Use NERVES temperature (0.3)
    trigger: "memory_section_exceeds_budget"
    recipe: "memory_compression.yaml"

  state_tracking:
    model: mind
    role: nerves  # Use NERVES temperature (0.3)
    trigger: "multi_goal_query"
    recipe: "state_tracking.yaml"

# =============================================================================
# VLLM SERVER CONFIGURATION (vLLM Tested)
# =============================================================================
vllm:
  host: "localhost"
  port: 8000
  tensor_parallel_size: 1
  gpu_memory_utilization: 0.90  # RTX 3090 24GB - can use higher utilization
  max_model_len: 4096  # Tested value
  quantization: "awq"  # AWQ 4-bit quantization
  enforce_eager: true  # Required on WSL

  # Startup command (tested):
  # python -m vllm.entrypoints.openai.api_server \
  #   --host 0.0.0.0 \
  #   --port 8000 \
  #   --model models/Qwen3-Coder-30B-AWQ \
  #   --served-model-name mind \
  #   --gpu-memory-utilization 0.90 \
  #   --max-model-len 4096 \
  #   --enforce-eager \
  #   --quantization awq \
  #   --trust-remote-code

  # Model loading (simplified - only MIND)
  startup_models:
    - mind  # Qwen3-Coder-30B-AWQ (~18GB) - serves ALL text roles

# =============================================================================
# FALLBACK CONFIGURATION (Simplified)
# =============================================================================
fallbacks:
  # MIND is the only text model - no fallbacks possible
  mind_fallback: null  # Cannot fallback Mind - critical
  eyes_fallback: null  # Skip vision if Eyes fails
  server_fallback: "mind"  # Use local MIND if server unavailable

# =============================================================================
# NOTES
# =============================================================================
# Simplified Stack (2026-01-06 - vLLM Tested):
# - MIND: Qwen3-Coder-30B-AWQ (~18GB, AWQ 4-bit) - ALL text roles
# - EYES: Qwen3-VL-2B (BF16, ~5GB) - vision tasks (cold swap)
# - Embedding: all-MiniLM-L6-v2 (CPU) - semantic search
#
# Hardware: RTX 3090 (24GB VRAM)
#
# NOT USED:
# - Qwen3-0.6B (REFLEX) - MIND handles classification tasks
# - Separate NERVES/VOICE models - MIND handles via temperature
#
# vLLM Configuration Notes:
# - quantization: awq (AWQ 4-bit)
# - --enforce-eager required on WSL
# - --gpu-memory-utilization 0.90 tested stable on RTX 3090
# - --max-model-len 4096 tested (can increase to 32768)
#
# Models available but not in active use:
# - models/Qwen3-0.6B (could use for ultra-fast classification)
# - models/Qwen3-VL-8B-Instruct (upgrade path for better vision)
#
# To swap models: Update model_id and local_path, restart vLLM
